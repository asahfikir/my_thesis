\chapter{PENDAHULUAN}

\section{Latar Belakang Penelitian}
% [19] Describe phenomenon gap and research gap.
Dunia kecerdasan buatan (\textit{Artificial Intelligence/AI}) mengalami transformasi radikal sejak diterbitkannya paper berjudul \textit{"Attention Is All You Need"} oleh \cite{vaswani2023attentionneed}. Paper tersebut memperkenalkan arsitektur \textit{Transformer}, yang menjadi fondasi munculnya fenomena lonjakan penggunaan AI. Sejak saat itu, perkembangan \textit{Large Language Models (LLM)} melonjak pesat, ditandai dengan kemunculan berbagai model generasi terbaru seperti \textit{GPT-4, Claude}, hingga \textit{PaLM}. Euforia ini membawa dampak signifikan, bukan hanya di kalangan peneliti akademis, tetapi juga merambah ke bidang industri dan dunia usaha.

Fenomena adopsi \textit{LLM} di sektor perusahaan saat ini menunjukkan tren yang sangat tinggi. Banyak perusahaan ingin mengimplementasikan \textit{LLM} sebagai asisten cerdas atau chatbot internal perusahaan. Bahkan beberapa perusahaan ada yang secara radikal mengganti peran manusia menggunakan \textit{LLM} sepenuhnya. Tujuannya adalah untuk meningkatkan efisiensi operasional, mulai dari layanan pelanggan otomatis, manajemen pengetahuan (knowledge management), hingga penyediaan dukungan keputusan berbasis data. Namun, di balik antusiasme ini, terdapat hambatan teknis yang tidak dapat diabaikan.

Penggunaan \textit{LLM} secara langsung seringkali tidak ideal untuk aplikasi kor-porat. LLM memiliki keterbatasan inheren berupa \textit{knowledge cutoff} (batas waktu pengetahuan), di mana model hanya memiliki informasi hingga waktu tertentu saat model tersebut dilatih. Selain itu, \textit{LLM} umumnya tidak memiliki akses terhadap data internal perusahaan yang bersifat privat, rahasia, atau terbaruâ€”seperti laporan keuangan terbaru, \textit{Standard Operating Procedure (SOP)} internal, maupun data proprietary lainnya. Hal ini menyebabkan jawaban yang dihasilkan oleh \textit{LLM} bisa menjadi usang, tidak relevan, atau bahkan menyesatkan dalam konteks spesifik perusahaan. Dan jika data ini digunakan untuk menjawab pada layanan pelanggan otomatis tentunya dapat berdampak negatif pada tingkat kepercayaan pengguna dan bahkan berdampak buruk pada nama brand.

Untuk mengatasi celah ini, pendekatan \textit{Retrieval-Augmented Generation (RAG)} menjadi standar de facto. \textit{RAG} bekerja dengan cara mengambil informasi relevan dari database dokumen eksternal terlebih dahulu, baru kemudian memberikannya kepada \textit{LLM} sebagai konteks untuk menjawab pertanyaan. Meskipun \textit{RAG} berhasil menjembatani kesenjangan informasi, implementasinya memunculkan tantangan baru. Akurasi sistem \textit{RAG} sangat bergantung pada bagaimana dokumen diproses sebelum diberikan kepada model.

\section{Identifikasi Masalah}
% [20] Clear, concrete, relevant to theory.
Dalam sistem \textit{RAG}, dokumen panjang harus dipecah menjadi potongan-potongan kecil yang disebut \textit{chunk}. Proses ini, yang dikenal sebagai \textit{chunking}, bukanlah proses yang trivial. Banyak penelitian dan praktisi seperti \cite{edge2025localglobalgraphrag} menemukan bahwa sistem RAG seringkali masih menghasilkan jawaban dengan akurasi yang kurang memuaskan pada kasus-kasus tertentu. Salah satu penyebab utamanya adalah parameter konfigurasi pada tahap pemecahan dokumen. Panjang chunk (\textit{chunk size}) yang tidak tepat dapat memotong konteks informasi menjadi terpotong-potong \cite{liu2023lostmiddlelanguagemodels}, sementara pengaturan tumpang tindih (\textit{overlap}) yang salah dapat menyebabkan redundansi informasi atau kegagalan dalam menyambungkan ide antar paragraf. Hal ini mendasari perlunya penelitian yang mengukur secara spesifik hubungan pengaruh panjang \textit{token (128, 256, 512)} dan persentase \textit{overlap (0\%, 10\%, 20\%)} terhadap tingkat akurasi jawaban.

Selain isu teknis terkait akurasi, terdapat pula kendala ekonomi dan infrastruktur dalam adopsi \textit{LLM}. Biaya komputasi untuk menjalankan model-model raksasa (seperti \textit{GPT-4}) melalui \textit{API} sangat tinggi dan membutuhkan biaya yang cukup besar untuk berlangganan. Selain itu, ketergantungan pada \textit{cloud computing} menimbulkan kekhawatiran terkait privasi data dan keamanan siber. Hal ini menjadi hambatan bagi perusahaan atau peneliti dengan sumber daya terbatas. Sebagai respon terhadap tantangan ini, muncul tren penggunaan \textit{Small Language Models (SLM)} seperti \textit{Microsoft Phi-2}, \textit{Google Gemma-2B}, dan \textit{Meta LLaMA 3.2}. Model-model ini memiliki ukuran yang jauh lebih kecil, memungkinkan untuk dijalankan secara lokal pada perangkat keras standar seperti laptop tanpa memerlukan GPU kelas server yang mahal. Namun, pertanyaannya adalah apakah pengorbanan ukuran model ini berbanding lurus dengan penurunan kualitas akurasi, terutama ketika dikombinasikan dengan strategi \textit{chunking} yang belum dioptimalkan.

Lebih jauh lagi, ketika melihat literatur penelitian terdahulu, sebagian besar studi mengenai optimalisasi \textit{RAG} dan pengaruh chunking masih sangat didominasi oleh penggunaan dataset berbahasa Inggris. Bahasa Indonesia, sebagai bahasa dengan struktur morfologi yang berbeda dan sifat bahasa yang agglutinative (menggabungkan kata dasar dengan afiks), memiliki karakteristik tokenisasi yang unik. Hasil temuan terbaik untuk bahasa Inggris belum tentu dapat diterapkan secara langsung dan memberikan hasil yang optimal pada dokumen berbahasa Indonesia. Selain itu sebagian besar studi juga lebih berfokus pada penggunakan model-model raksasa seperti \textit{GPT-4} \cite{xu2024activecellbalancingextended} yang membutuhkan komputasi serta biaya yang tidak sedikit. Berdasarkan uraian tersebut, penelitian berjudul "\textit{Analisis Pengaruh Panjang Chunk dan Overlap terhadap Akurasi Jawaban Small Language Models (SLM) pada Dokumen Publik Berbahasa Indonesia}" ini menjadi sangat penting untuk dilakukan guna menjawab tantangan akurasi, efisiensi biaya, dan relevansi lokal dalam ekosistem AI saat ini.

\section{Perumusan Masalah}
% [21] Question format (?).
Berdasarkan uraian latar belakang diatas, rumusan masalah dalam penelitian ini adalah:
\begin{enumerate}
    \item Bagaimana pengaruh variasi panjang chunk (128, 256, 512 token) terhadap akurasi jawaban \textit{SLM (Phi-2, Gemma-2B, LLaMA 3.2)} pada dokumen publik berbahasa Indonesia?
    \item Bagaimana pengaruh variasi persentase \textit{overlap} (0\%, 10\%, 20\%) terhadap akurasi jawaban \textit{SLM}?
    \item Konfigurasi \textit{chunk size} dan \textit{overlap} manakah yang menghasilkan performa terbaik untuk masing-masing model \textit{SLM} dalam hal akurasi semantik (\textit{Cosine Similarity}) dan keakuratan kata (\textit{F1-Score/ROUGE})?
    \item Bagaimana perbandingan performa ketiga model \textit{SLM} tersebut di bawah kendala perangkat keras laptop standar?
\end{enumerate}


\section{Tujuan Penelitian}
% [22]
Adapun tujuan penelitian ini adalah:

\begin{enumerate}
    \item Menganalisis pengaruh signifikan panjang chunk dan overlap terhadap metrik evaluasi (\textit{F1, ROUGE, dan Cosine Similarity}).
    \item Menentukan konfigurasi parameter chunking optimal untuk dokumen berbahasa Indonesia.
    \item Membandingkan efektivitas model \textit{Phi-2, Gemma-2B, dan LLaMA 3.2} dalam tugas \textit{Question Answering} berbasis \textit{RAG} pada lingkungan komputasi terbatas.
\end{enumerate}

\section{Manfaat Penelitian}
% [23] Theoretical and Practical.
\subsection{Manfaat Teoritis}
Memberikan kontribusi pada literatur mengenai sensitivitas \textit{SLM} terhadap strategi pemrosesan teks (\textit{chunking}) dalam bahasa Indonesia.
\subsection{Manfaat Praktis}
Memberikan panduan bagi pengembang dan peneliti dalam membangun sistem \textit{RAG} khususnya domain dokumen berbahasa Indonesia yang efisien dan akurat menggunakan perangkat keras konsumen (laptop) tanpa ketergantungan pada \textit{API} berbayar.

%%% Local Variables:
%%% TeX-master: "main"
%%% End:
